{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Camily Milsoni Rodrigues**"
      ],
      "metadata": {
        "id": "ZTNi-P1Khyco"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **EXERCÍCIO 1:**\n",
        "\n",
        "* Construir uma aplicação (notebook Colab) de gerador de códigos (uma função) com o MAMBA CODESTRAL (MISTRAL) de IA que gere um código em linguagem C."
      ],
      "metadata": {
        "id": "fcLFSEu8FbUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalação da biblioteca *mistralai*:"
      ],
      "metadata": {
        "id": "YEl2jyiAaJ-g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuj9JnMPE_7X",
        "outputId": "cf417acc-cabf-4bcd-df89-281cb60820c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mistralai in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
            "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from mistralai) (0.2.0)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from mistralai) (0.27.2)\n",
            "Requirement already satisfied: jsonpath-python<2.0.0,>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from mistralai) (1.0.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from mistralai) (2.9.2)\n",
            "Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.10/dist-packages (from mistralai) (2.8.2)\n",
            "Requirement already satisfied: typing-inspect<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from mistralai) (0.9.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil==2.8.2->mistralai) (1.16.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->mistralai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->mistralai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->mistralai) (1.0.6)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->mistralai) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->mistralai) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->mistralai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.9.0->mistralai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.9.0->mistralai) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.9.0->mistralai) (4.12.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<0.10.0,>=0.9.0->mistralai) (1.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.28.0,>=0.27.0->mistralai) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install mistralai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilização da biblioteca *mistralai* para interagir com o modelo Mistral e gerar uma função em C que calcula o fatorial de um número. Primeiro, a chave de API é configurada para autenticar a comunicação com o modelo. Em seguida, é definida a mensagem de entrada, onde o usuário solicita a criação da função. Através do método *chat.complete*, o código envia essa solicitação ao modelo, que retorna uma resposta com a implementação em C. Por fim, a função gerada é exibida utilizando o comando *print()*:"
      ],
      "metadata": {
        "id": "JsFuVGPgaQlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from mistralai import Mistral\n",
        "\n",
        "api_key = \"Dc9wm61bGIgjh4cT03Ma2Vwv4UMddAhQ\"\n",
        "\n",
        "client = Mistral(api_key=api_key)\n",
        "\n",
        "model = \"codestral-latest\"\n",
        "\n",
        "message = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Write C code function to calculate the factorial of a number\"\n",
        "    }\n",
        "]\n",
        "\n",
        "chat_response = client.chat.complete(\n",
        "    model=model,\n",
        "    messages=message\n",
        ")\n",
        "print(chat_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0T_0Th2QFYBb",
        "outputId": "cbbd3ddf-48bf-42e2-f9c9-aed909a1489f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, here's a simple recursive function in C to calculate the factorial of a number.\n",
            "\n",
            "```c\n",
            "#include <stdio.h>\n",
            "\n",
            "unsigned long long factorial(unsigned int n) {\n",
            "    if (n == 0)\n",
            "        return 1;\n",
            "    else\n",
            "        return n * factorial(n - 1);\n",
            "}\n",
            "\n",
            "int main() {\n",
            "    unsigned int num;\n",
            "    printf(\"Enter a number: \");\n",
            "    scanf(\"%u\", &num);\n",
            "    printf(\"Factorial of %u = %llu\\n\", num, factorial(num));\n",
            "    return 0;\n",
            "}\n",
            "```\n",
            "\n",
            "This function works by recursively calling itself, each time with a smaller number, until it reaches 0. The base case of the recursion is when `n` equals 0, in which case the function returns 1.\n",
            "\n",
            "Please note that this function uses `unsigned long long` to store the factorial result, which can hold values up to 18,446,744,073,709,551,615 (2^64 - 1). This is sufficient for factorials of numbers up to 20, as 20! is 2,432,902,008,176,640,000. If you need to calculate the factorial of larger numbers, you'll need to use a different data type or a library that supports arbitrary-precision arithmetic.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **EXERCÍCIO 2:**\n",
        "\n",
        "* Construir uma aplicação (notebook Colab) para transcrever os dados de um recibo (imagem)."
      ],
      "metadata": {
        "id": "lCgrcGydH1O7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalação da biblioteca *pytesseract* (interface Python para o *Tesseract*) e do próprio *Tesseract OCR* e instalação da biblioteca *requests*, que facilita o download de imagens e interação com APIs:"
      ],
      "metadata": {
        "id": "nnEBMVetazdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract\n",
        "!apt-get install tesseract-ocr\n",
        "!pip install requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKRaakO5TpAA",
        "outputId": "e7eb852a-daf8-46ee-c934-fe7f490a24a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (10.4.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importação das bibliotecas necessárias para realizar OCR em imagens: *pytesseract* para o reconhecimento de texto, *PIL* para manipulação de imagens, *OpenCV* para pré-processamento, *NumPy* para operações com arrays, *requests* para baixar imagens da web e *BytesIO* para manipular imagens diretamente na memória:"
      ],
      "metadata": {
        "id": "8uCDAgjjbGia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytesseract\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import BytesIO"
      ],
      "metadata": {
        "id": "i6Clt54BTqHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definição de duas funções para processar e transcrever texto de recibos em imagens. A função *preprocess_image* recebe um caminho de arquivo ou uma URL de imagem, baixa ou carrega a imagem e a converte para escala de cinza, facilitando o processamento pelo OCR. A função *transcribe_receipt* também lida com URLs ou caminhos de arquivos e utiliza a biblioteca *pytesseract* para converter o conteúdo da imagem em texto. O exemplo utiliza uma URL de imagem de um recibo, aplica o pré-processamento e, em seguida, transcreve o texto extraído da imagem:"
      ],
      "metadata": {
        "id": "KzjkAktxbHQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image_path_or_url):\n",
        "    if image_path_or_url.startswith('http'):\n",
        "        response = requests.get(image_path_or_url)\n",
        "        img_array = np.array(bytearray(response.content), dtype=np.uint8)\n",
        "        image = cv2.imdecode(img_array, cv2.IMREAD_GRAYSCALE)\n",
        "    else:\n",
        "        image = cv2.imread(image_path_or_url, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    return image\n",
        "\n",
        "def transcribe_receipt(image_path_or_url):\n",
        "    if image_path_or_url.startswith('http'):\n",
        "        response = requests.get(image_path_or_url)\n",
        "        img = Image.open(BytesIO(response.content))\n",
        "    else:\n",
        "        img = Image.open(image_path_or_url)\n",
        "\n",
        "    text = pytesseract.image_to_string(img)\n",
        "    return text\n",
        "\n",
        "image_url = 'https://www.klippa.com/wp-content/uploads/2020/02/ocr-for-receipts.jpg'\n",
        "\n",
        "preprocessed_image = preprocess_image(image_url)\n",
        "\n",
        "transcribed_text = transcribe_receipt(image_url)\n",
        "\n",
        "print(transcribed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fAjXOQLTYEk",
        "outputId": "914598cb-4613-4b65-9194-9938e613d11e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "\n",
            "Keizersgracht 504\n",
            "1017 EJ Amsterdam\n",
            "tel; 020 3305508\n",
            "\n",
            "REKENING\n",
            "\n",
            "04-12-2019 Kelner: Niko\n",
            "\n",
            "14:57 Tafel: 18\n",
            "Omschrijving Aantal Prijs  Totaal\n",
            "Flat white (te Sie € 3575\n",
            "Acai Juice XL ie 5:95eee 5,95\n",
            "Jungle Juice te 4,15 € 4,15\n",
            "Jus XL te 4,35 € 4,35\n",
            "Chocome|lt Melk Sla eee Sy 7a) re 9,48\n",
            "Paddoburger 1e 6,45 € 6,45\n",
            "Tonijnsalade 1 e 6,60 € 6,60\n",
            "\n",
            "Totaal: € 35,00\n",
            "\n",
            "BIW: € 2,89 (9,00%)\n",
            "BIW totaal; € 2,89\n",
            "\n",
            "Wu. bage)sbeans.n]\n",
            "\f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **EXERCÍCIO 3:**\n",
        "\n",
        "* Construir uma aplicação (notebook Colab) para descrever os dados de um gráfico."
      ],
      "metadata": {
        "id": "psMMlXFMLw9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importação do módulo *os* para interagir com o sistema operacional e a classe Mistral da biblioteca *mistralai* para usar a API do modelo Mistral:"
      ],
      "metadata": {
        "id": "7mUuIbAGbxb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from mistralai import Mistral"
      ],
      "metadata": {
        "id": "D89plJdKSOO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definição da chave de API para autenticação e criação de um cliente para interagir com a API do modelo Mistral. A variável *api_key* armazena a chave de autenticação, enquanto *client* é uma instância da classe Mistral, configurada com essa chave. A variável *model* define o nome do modelo a ser utilizado, neste caso, o \"pixtral-12b-2409\":"
      ],
      "metadata": {
        "id": "AwBzHNNib38d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = \"Dc9wm61bGIgjh4cT03Ma2Vwv4UMddAhQ\"\n",
        "client = Mistral(api_key=api_key)\n",
        "model = \"pixtral-12b-2409\""
      ],
      "metadata": {
        "id": "KBXF580jSSXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Envio de uma solicitação ao modelo Mistral para analisar uma imagem e responder à pergunta \"What’s in this image?\". Ele utiliza o link da imagem de um gráfico de barras e imprime a descrição gerada pelo modelo:"
      ],
      "metadata": {
        "id": "1dummdPOcH2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\n",
        "                \"type\": \"text\",\n",
        "                \"text\": \"What's in this image?\"\n",
        "            },\n",
        "            {\n",
        "                \"type\": \"image_url\",\n",
        "                \"image_url\": \"https://statisticsbyjim.com/wp-content/uploads/2021/06/bar_chart_clustered.png\"\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "chat_response = client.chat.complete(\n",
        "    model=model,\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "print(chat_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOgufjiCRvrb",
        "outputId": "2ee19fd6-7b02-48c0-95f3-36dd51d70506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The image is a bar chart titled \"Flavor Preferences by Gender.\" It displays the preferences of females and males for three different flavors: Chocolate, Strawberry, and Vanilla. The y-axis represents the count, while the x-axis represents the gender and flavor combinations.\n",
            "\n",
            "Here is a detailed summary of the findings based on each flavor:\n",
            "\n",
            "1. **Chocolate:**\n",
            "   - Females: Approximately 36\n",
            "   - Males: Approximately 20\n",
            "\n",
            "   Females show a significantly higher preference for chocolate compared to males.\n",
            "\n",
            "2. **Strawberry:**\n",
            "   - Females: Approximately 17\n",
            "   - Males: Approximately 19\n",
            "\n",
            "   Males have a slightly higher preference for strawberry than females, but the difference is not very substantial.\n",
            "\n",
            "3. **Vanilla:**\n",
            "   - Females: Approximately 11\n",
            "   - Males: Approximately 32\n",
            "\n",
            "   Males exhibit a much stronger preference for vanilla compared to females.\n",
            "\n",
            "The chart visually conveys that females prefer chocolate the most, males prefer vanilla the most, and there is a balanced preference for strawberry between the two genders.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **EXERCÍCIO 4:**\n",
        "\n",
        "* Construir uma aplicação (notebook Colab) para geração de texto (prompt) de uma temática com cadeia de pensamento."
      ],
      "metadata": {
        "id": "KnRU5CsYSeQR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importação das classes *GPTNeoForCausalLM* e *GPT2Tokenizer* da biblioteca *transformers*. A primeira é usada para carregar o modelo GPT-Neo para geração de texto, e a segunda para tokenizar o texto, convertendo-o em tokens processáveis pelo modelo:"
      ],
      "metadata": {
        "id": "DwXIzSRzcgWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPTNeoForCausalLM, GPT2Tokenizer"
      ],
      "metadata": {
        "id": "IhWQWZ5fYeK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carregamento do modelo *GPT-Neo 1.3B* e seu tokenizador da *EleutherAI* usando a biblioteca *transformers*. O tokenizador converte o texto em tokens, enquanto o modelo é usado para gerar ou completar texto com base nas entradas fornecidas:"
      ],
      "metadata": {
        "id": "oqNkiHqjcsO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('EleutherAI/gpt-neo-1.3B')\n",
        "model = GPTNeoForCausalLM.from_pretrained('EleutherAI/gpt-neo-1.3B')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuU5-WIsYfQ1",
        "outputId": "0eaab066-fd56-42f7-b9dd-94a15e78f55a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definição da função *gerar_texto_neo*, que usa o modelo *GPT-Neo* para gerar texto com base em um tema fornecido. O texto gerado é controlado pelo comprimento máximo, diversidade (temperatura) e evita repetições de n-grams. No exemplo, o tema \"Inteligência artificial no futuro do trabalho\" é usado para gerar e imprimir o conteúdo:"
      ],
      "metadata": {
        "id": "YkFcwQ3sdFrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gerar_texto_neo(tema, max_length=200):\n",
        "    prompt = f\"Pense sobre o tema '{tema}' e explique de forma lógica e progressiva:\"\n",
        "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
        "    outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2, temperature=0.7)\n",
        "    texto_gerado = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return texto_gerado\n",
        "\n",
        "tema = \"Inteligência artificial no futuro do trabalho\"\n",
        "texto_gerado = gerar_texto_neo(tema)\n",
        "print(texto_gerado)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-upQGZaYbWG",
        "outputId": "0db98189-b2d0-4e4f-859f-58fe3eb2b867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pense sobre o tema 'Inteligência artificial no futuro do trabalho' e explique de forma lógica e progressiva:\n",
            "\n",
            "O que é inteligente?\n",
            "A inteligencia artificial é um conceito que segue a língua da ciências humanas. É um termo que, em inglês, significa \"capacidade de pensar\".\n",
            "Este conceito é muito mais complexo do que isso.\n",
            "Para entender o conceitu, é preciso entendê-lo de uma formidável maneira. Este é o objetivo da minha pesquisa. A inteligentia é umas coisas que são mágicas, mas que não só sêmplos de intelição, como também de cren\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Utilizando outro modelo (GPT2):**\n"
      ],
      "metadata": {
        "id": "Tke4y-D7cVvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importação do *GPT2LMHeadModel* e do *GPT2Tokenizer* da biblioteca *transformers*. O modelo GPT-2 é usado para gerar texto, enquanto o tokenizador converte o texto em tokens que o modelo pode processar e reconverte os tokens em texto legível:"
      ],
      "metadata": {
        "id": "9gjXql-Fdc21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ],
      "metadata": {
        "id": "civok6_fZUuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carregamento do modelo *GPT-2* e seu tokenizador pré-treinado usando a biblioteca *transformers*, permitindo gerar e processar texto com base em entradas fornecidas:"
      ],
      "metadata": {
        "id": "QVVOAb1cdqR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "NfeCPWdpZWVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definição da função *gerar_texto*, que usa o modelo *GPT-2* para gerar um texto explicativo sobre um tema fornecido. O texto é gerado com base em um prompt criado a partir do tema, e o comprimento, repetição e diversidade do conteúdo são controlados. No exemplo, o tema \"Mudanças climáticas e suas implicações para o futuro\" é usado para gerar o texto:"
      ],
      "metadata": {
        "id": "aRg98NVYd7dS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gerar_texto(tema, max_length=200):\n",
        "    prompt = f\"Pense sobre o tema '{tema}' e explique de forma lógica e progressiva:\"\n",
        "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
        "    outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2, temperature=0.7)\n",
        "    texto_gerado = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return texto_gerado\n",
        "\n",
        "tema = \"Mudanças climáticas e suas implicações para o futuro\"\n",
        "texto_gerado = gerar_texto(tema)\n",
        "\n",
        "print(texto_gerado)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFUpZfrBZY2H",
        "outputId": "a54722ef-cdc6-4ff1-f707-a2f2b4504088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pense sobre o tema 'Mudanças climáticas e suas implicações para o futuro' e explique de forma lógica e progressiva:\n",
            "\n",
            "\"The first step is to understand the nature of the problem. The second step, which is the most important, is that of understanding the process of development. This is a very important step. It is not a matter of 'how' or 'when' but of how the development of a country is going to be. In the case of developing countries, the first stage is development, and the second stage, development is called 'developmental development'. The development process is very different from the one in which the people are developing. Development is different in the sense that it is more or less a process. But the same thing is true of other things. For example, in developing nations, there is no development in terms of social development or economic development; it\n"
          ]
        }
      ]
    }
  ]
}